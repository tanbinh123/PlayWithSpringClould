# 消息队列

关于消息队列的问题：

- 为什么使用消息队列？
- 消息队列有什么优点和缺点？
- Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么区别，以及适合哪些场景？

### 为什么使用消息队列

回答这个问题从**业务场景**这个角度去回答这个问题是个不错的选择。

消息队列的使用场景比较核心的有 3 个： **解耦**、**异步**、**削峰**

> 通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，系统就跟其它系统彻底解耦了。

当系统中某个业务写库的操作比较繁多的时候，可以考虑使用队列异步入库（不影响业务的写操作），来加快响应请求速度。

然后大家知道系统中的数据库是存在瓶颈的，打个比方：某活动 A 系统中 MySQL 1s 时间最多能处理 2k 个，活动发布半小时内预计 1s 的请求数量远远超过 2k，平均 8k/s。

这个时候我们就可以使用到消息队列，让请求先进 MQ，然后控制 A 系统中 MySQL 每秒钟从 MQ 中拉取 2k 个消息进行消费。

### 消息队列的优缺点

优点差不多就是上面提到的消息队列带来的好处： **解耦**、**异步**、**削峰**

缺点的话，添加了外部依赖之后：

- 系统可用性降低
- 系统复杂度提高
- 一致性问题

> 那么问题来了，如何让消息队列高可用？

### Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？

| 特性 | ActiveMQ | RabbitMQ | RocketMQ | Kafka |
|---|---|---|---|---|
| 单机吞吐量 | 万级，比 RocketMQ、Kafka 低一个数量级 | 同 ActiveMQ | 10 万级，支撑高吞吐 | 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 |
| topic 数量对吞吐量的影响 | | | topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic | topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 |
| 时效性 | ms 级 | 微秒级，这是 RabbitMQ 的一大特点，延迟最低 | ms 级 | 延迟在 ms 级以内 |
| 可用性 | 高，基于主从架构实现高可用 | 同 ActiveMQ | 非常高，分布式架构 | 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 |
| 消息可靠性 | 有较低的概率丢失数据 | 基本不丢 | 经过参数优化配置，可以做到 0 丢失 | 同 RocketMQ |
| 功能支持 | MQ 领域的功能极其完备 | 基于 erlang 开发，并发能力很强，性能极好，延时很低 | MQ 功能较为完善，还是分布式的，扩展性好 | 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 |


### RabbitMQ 的高可用

RabbitMQ 是比较有代表性的，因为是**基于主从**（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。

RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。

**单机模式**：Demo 级别

**普通集群模式（非高可用）**：多台机器上启动多个 RabbitMQ 实例。创建的 queue ，只会放在其中 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。
> 该模式只能提高 RabbitMQ 的吞吐量，并没有高可用性

**镜像集群模式（高可用）**：跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。

### 如何保证消息不被重复消费

- 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
- 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。
- 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。
- 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。

